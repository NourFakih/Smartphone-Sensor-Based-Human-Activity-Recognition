# Smartphone-Sensor-Based-Human-Activity-Recognition

## Human Activity Recognition with Feature Engineering and Deep Fusion

## ğŸ“Œ Motivation

Human Activity Recognition (HAR) using wearable sensors is a critical component in various applications, from health monitoring to smart environments. While deep learning has gained attention for its representation power, classical machine learning pipelinesâ€”especially when powered by strong signal processing and feature engineeringâ€”can still outperform deep models on moderately sized datasets. This project investigates the performance trade-off between handcrafted feature pipelines and deep neural models using the UCI HAR dataset.

---

## ğŸ—‚ï¸ Repository Contents

This repository includes three Jupyter notebooks:

1. ğŸ“˜â€¯**FeatureEngineering+model.ipynb**
   This notebook implements a complete feature engineering pipeline to reduce the original 561 features to a compact set of 100 features. The following 10 pipelines were trained and evaluated on these features:

   * âœ…â€¯Decision Tree â€“ 100.0%
   * âœ…â€¯XGBoost (Full) â€“ 100.0%
   * âœ…â€¯Linear SVC on PCA Features â€“ 93.8%
   * âœ…â€¯Linear SVC on Time-Domain Only â€“ 93.6%
   * âœ…â€¯Linear SVC on Frequency-Domain Only â€“ 83.4%
   * âœ…â€¯LSTM Classifier â€“ 99.86%
   * âœ…â€¯Physics-Informed LSTM (PINN) â€“ 84.76%
   * âœ…â€¯Transformer Encoder â€“ 93.72%
   * âœ…â€¯Autoregressive (AR) + SVM â€“ 93.0%
   * âœ…â€¯Variational Autoencoder (VAE) + SVM â€“ 66.0%

   Feature engineering steps include duplicate feature removal, variance thresholding, multicollinearity filtering using Pearson correlation, and selection of the top 100 features via ANOVA F-statistics.

2. ğŸ“˜â€¯**NOFeatureEngineering+model.ipynb**
   This notebook replicates the model training process using the complete unfiltered set of 561 features. It enables side-by-side comparisons to evaluate the benefit of feature selection and dimensionality reduction. Hereâ€™s a summary of test performance for the same 10 models:

   * Decision Tree â€“ 100.0%
   * XGBoost â€“ 100.0%
   * Linear SVC on PCA Features â€“ 96.2%
   * Linear SVC on Time-Domain Only â€“ 96.1%
   * Linear SVC on Frequency-Domain Only â€“ 93.8%
   * LSTM â€“ 99.23%
   * PINN â€“ 88.12%
   * Transformer â€“ 93.72%
   * AR + SVM â€“ 95.0%
   * VAE + SVM â€“ 78.0%

   This notebook reveals that while raw features can support strong classifiers, reduced features yield models that are lighter and potentially more robust to noise or overfitting.

3. ğŸ“˜â€¯**FFt+models\_(3).ipynb**
   This notebook works directly with the raw 3D accelerometer and gyroscope signals, applying FFT-based feature extraction. It investigates MLP-based models on these frequency-domain representations. While the results (\~95% with MLP) are promising, they were not included in the formal paper to avoid overcrowding the comparison. Nevertheless, they reinforce the viability of using raw-to-FFT pipelines for HAR tasks.

---

## ğŸ“ Project Summary

We benchmarked classical and deep models using both engineered and unfiltered feature sets. Our experiments show that:

* Handcrafted features with XGBoost achieved the highest overall test accuracy (100%) and macro-F1 score (0.992).
* LSTM models benefited from expert features, scoring up to 99.86% with engineering vs. 99.23% without.
* Models like Transformer and PINN performed decently (84â€“94%) but were sensitive to input dimensionality and training volume.
* VAE-based pipelines lagged significantly, suggesting that reconstruction-driven representations may not align well with HAR classification goals.
* Frequency-only models underperformed compared to time-only and full models, but retained moderate predictive power (\~83â€“94%).

These findings align with the broader literature: on small-to-medium HAR datasets, classical pipelines with well-chosen features remain extremely competitiveâ€”even compared to modern neural models.

---

## ğŸ“š Citation

If you use this dataset or replicate our pipelines, please cite:

```bibtex
@misc{reyes2013har,
  author       = {Reyes-Ortiz, Jorge and Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Parra, Xavier},
  title        = {Human Activity Recognition Using Smartphones [Dataset]},
  year         = {2013},
  publisher    = {UCI Machine Learning Repository},
  howpublished = {\url{https://doi.org/10.24432/C54S4K}},
  note         = {Accessed: 2025-05-04}
}
```
Great! Here's how you can acknowledge the Kaggle notebooks that supported your work in your README under a new Acknowledgments section:

---

## ğŸ™ Acknowledgments

We would like to thank the authors of the following Kaggle notebooks, whose insights and implementations significantly helped guide our development and experimentation for this project:

* ğŸ“˜â€¯"What does your smartphone know about you?" by morrisb
  [https://www.kaggle.com/code/morrisb/what-does-your-smartphone-know-about-you/notebook](https://www.kaggle.com/code/morrisb/what-does-your-smartphone-know-about-you/notebook)

* ğŸ“˜â€¯"SVM for Multiclass Classification" by pranathichunduru
  [https://www.kaggle.com/code/pranathichunduru/svm-for-multiclass-classification](https://www.kaggle.com/code/pranathichunduru/svm-for-multiclass-classification)

* ğŸ“˜â€¯"Human Activity Recognition: Scientific Perspective" by essammohamed4320
  [https://www.kaggle.com/code/essammohamed4320/human-activity-recognition-scientific-prespective#Time-Domain](https://www.kaggle.com/code/essammohamed4320/human-activity-recognition-scientific-prespective#Time-Domain)

* ğŸ“˜â€¯"Human Activity Recognition - LSTM" by prasadmeesala
  [https://www.kaggle.com/code/prasadmeesala/human-activity-recognition-lstm](https://www.kaggle.com/code/prasadmeesala/human-activity-recognition-lstm)

Their thoughtful analyses, modeling strategies, and clean implementation were instrumental in shaping the final structure of our experiments.

---



