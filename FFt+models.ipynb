{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/archive.zip -d /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck5oIopBwrvr",
        "outputId": "f1e0f0e3-613f-4b3c-a243-45e0e82be0f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/archive.zip\n",
            "replace /content/test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7lwm1XEdvRi",
        "outputId": "f058b478-ab96-4fee-c9c0-383fdeb190cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading raw UCI HAR dataset…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-8-8727767901c9>:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (7352, 280)\n",
            "X_test  shape: (2947, 280)\n"
          ]
        }
      ],
      "source": [
        "# ─── 0) Install prerequisites (Colab only) ─────────────────────────────────────\n",
        "# !pip install requests scipy\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.fft import rfft\n",
        "import torch\n",
        "\n",
        "# ─── 1) Download & unzip UCI HAR raw signals ───────────────────────────────────\n",
        "UCI_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\"\n",
        "ZIP_PATH  = \"UCI_HAR.zip\"\n",
        "DATA_DIR  = \"UCI_HAR_Dataset\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(\"Downloading raw UCI HAR dataset…\")\n",
        "    r = requests.get(UCI_URL)\n",
        "    with open(ZIP_PATH, \"wb\") as f:\n",
        "        f.write(r.content)\n",
        "    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
        "        z.extractall()      # creates folder 'UCI HAR Dataset'\n",
        "    os.rename(\"UCI HAR Dataset\", DATA_DIR)\n",
        "\n",
        "# ─── 2) Load your feature CSVs (563 features) ─────────────────────────────────\n",
        "train_df = pd.read_csv(\"./train.csv\")  # adjust path if needed\n",
        "test_df  = pd.read_csv(\"./test.csv\")\n",
        "\n",
        "# ─── 3) Identify your 100 reduced features ────────────────────────────────────\n",
        "# (If you have an explicit list, plug it in here; else auto‐select the first 100 non‐label cols)\n",
        "all_feats = [c for c in train_df.columns if c != \"Activity\"]\n",
        "reduced_feats = all_feats[:100]  # replace this with your actual 100‐feature list\n",
        "\n",
        "# ─── 4) Load raw inertial signals into numpy arrays ───────────────────────────\n",
        "def load_raw_windows(split=\"train\"):\n",
        "    base = os.path.join(DATA_DIR, split, \"Inertial Signals\")\n",
        "    mats = []\n",
        "    for signal in [\"total_acc\", \"body_acc\", \"body_gyro\"]:\n",
        "        for axis in [\"x\",\"y\",\"z\"]:\n",
        "            fn = f\"{signal}_{axis}_{split}.txt\"\n",
        "            path = os.path.join(base, fn)\n",
        "            arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
        "            mats.append(arr[..., np.newaxis])\n",
        "    # result: (n_windows, 128, 6)\n",
        "    return np.concatenate(mats, axis=2)\n",
        "\n",
        "raw_train = load_raw_windows(\"train\")\n",
        "raw_test  = load_raw_windows(\"test\")\n",
        "\n",
        "# ─── 5) Compute FFT features (20 bins × 6 channels = 120 dims) ────────────────\n",
        "def compute_fft_features(X, n_bins=20, drop_dc=True):\n",
        "    # X: (n_windows, 128, 6)\n",
        "    fft_vals = np.abs(rfft(X, axis=1))       # → (n_windows, 65, 6)\n",
        "    if drop_dc:\n",
        "        fft_vals = fft_vals[:, 1:n_bins+1, :] # drop DC term\n",
        "    else:\n",
        "        fft_vals = fft_vals[:, :n_bins, :]\n",
        "    return fft_vals.reshape(X.shape[0], -1)   # → (n_windows, 6*n_bins)\n",
        "\n",
        "fft_train = compute_fft_features(raw_train, n_bins=20)\n",
        "fft_test  = compute_fft_features(raw_test,  n_bins=20)\n",
        "\n",
        "# ─── 6) Merge FFT + your 100 reduced features ─────────────────────────────────\n",
        "# Extract labels\n",
        "y_train = train_df[\"Activity\"].values\n",
        "y_test  = test_df[\"Activity\"].values\n",
        "\n",
        "# Build final feature matrices\n",
        "X_train = np.hstack([train_df[reduced_feats].values, fft_train])  # shape (7352, 100+120)\n",
        "X_test  = np.hstack([ test_df[ reduced_feats].values, fft_test ])  # shape (2947, 100+120)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test  shape:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommended Models for FFT Features + Example Training Code\n",
        "\n",
        "import time\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Assumes X_train, X_test, y_train, y_test are already defined in your notebook\n",
        "\n",
        "models = {\n",
        "    \"SVM (RBF kernel)\": SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\"),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"HistGradientBoosting\": HistGradientBoostingClassifier(random_state=42),\n",
        "    \"MLP (Feedforward NN)\": MLPClassifier(hidden_layer_sizes=(128, 64),\n",
        "                                          activation=\"relu\",\n",
        "                                          solver=\"adam\",\n",
        "                                          max_iter=200,\n",
        "                                          random_state=42)\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, clf in models.items():\n",
        "    print(f\"\\n=== Training & Evaluating: {name} ===\")\n",
        "\n",
        "    # Training\n",
        "    start_train = time.time()\n",
        "    clf.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_train\n",
        "\n",
        "    # Inference\n",
        "    start_test = time.time()\n",
        "    y_pred = clf.predict(X_test)\n",
        "    test_time = time.time() - start_test\n",
        "\n",
        "    # Metrics\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "    # Optional: Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "    print(f\"Training time: {train_time:.2f}s   Inference time: {test_time:.2f}s\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    results.append((name, acc, train_time, test_time))\n",
        "\n",
        "# Summary table\n",
        "import pandas as pd\n",
        "summary = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Train Time (s)\", \"Inference Time (s)\"])\n",
        "print(\"\\n=== Summary ===\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSeoLOICd6E3",
        "outputId": "42dce472-b4b5-4180-f07b-a173116cde0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training & Evaluating: SVM (RBF kernel) ===\n",
            "Accuracy: 0.9393\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "            LAYING       1.00      1.00      1.00       537\n",
            "           SITTING       0.89      0.84      0.86       491\n",
            "          STANDING       0.86      0.90      0.88       532\n",
            "           WALKING       0.99      0.99      0.99       496\n",
            "WALKING_DOWNSTAIRS       0.97      0.92      0.94       420\n",
            "  WALKING_UPSTAIRS       0.94      0.98      0.96       471\n",
            "\n",
            "          accuracy                           0.94      2947\n",
            "         macro avg       0.94      0.94      0.94      2947\n",
            "      weighted avg       0.94      0.94      0.94      2947\n",
            "\n",
            "Confusion Matrix:\n",
            " [[537   0   0   0   0   0]\n",
            " [  1 413  77   0   0   0]\n",
            " [  0  53 479   0   0   0]\n",
            " [  0   0   0 489   7   0]\n",
            " [  0   0   0   3 387  30]\n",
            " [  0   0   0   1   7 463]]\n",
            "Training time: 1.97s   Inference time: 3.19s\n",
            "--------------------------------------------------\n",
            "\n",
            "=== Training & Evaluating: Random Forest ===\n",
            "Accuracy: 0.9226\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "            LAYING       1.00      1.00      1.00       537\n",
            "           SITTING       0.86      0.85      0.86       491\n",
            "          STANDING       0.86      0.88      0.87       532\n",
            "           WALKING       0.95      0.98      0.96       496\n",
            "WALKING_DOWNSTAIRS       0.97      0.88      0.92       420\n",
            "  WALKING_UPSTAIRS       0.90      0.94      0.92       471\n",
            "\n",
            "          accuracy                           0.92      2947\n",
            "         macro avg       0.92      0.92      0.92      2947\n",
            "      weighted avg       0.92      0.92      0.92      2947\n",
            "\n",
            "Confusion Matrix:\n",
            " [[537   0   0   0   0   0]\n",
            " [  0 416  75   0   0   0]\n",
            " [  0  65 467   0   0   0]\n",
            " [  0   0   0 485   5   6]\n",
            " [  0   0   0   8 369  43]\n",
            " [  0   0   0  19   7 445]]\n",
            "Training time: 12.31s   Inference time: 0.04s\n",
            "--------------------------------------------------\n",
            "\n",
            "=== Training & Evaluating: HistGradientBoosting ===\n",
            "Accuracy: 0.9308\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "            LAYING       1.00      1.00      1.00       537\n",
            "           SITTING       0.89      0.80      0.84       491\n",
            "          STANDING       0.83      0.91      0.87       532\n",
            "           WALKING       0.97      0.99      0.98       496\n",
            "WALKING_DOWNSTAIRS       0.99      0.90      0.94       420\n",
            "  WALKING_UPSTAIRS       0.92      0.97      0.95       471\n",
            "\n",
            "          accuracy                           0.93      2947\n",
            "         macro avg       0.93      0.93      0.93      2947\n",
            "      weighted avg       0.93      0.93      0.93      2947\n",
            "\n",
            "Confusion Matrix:\n",
            " [[537   0   0   0   0   0]\n",
            " [  0 393  98   0   0   0]\n",
            " [  0  48 484   0   0   0]\n",
            " [  0   0   0 492   0   4]\n",
            " [  0   0   0   5 379  36]\n",
            " [  0   0   0   8   5 458]]\n",
            "Training time: 17.89s   Inference time: 0.31s\n",
            "--------------------------------------------------\n",
            "\n",
            "=== Training & Evaluating: MLP (Feedforward NN) ===\n",
            "Accuracy: 0.9542\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "            LAYING       0.99      1.00      0.99       537\n",
            "           SITTING       0.97      0.86      0.91       491\n",
            "          STANDING       0.89      0.97      0.93       532\n",
            "           WALKING       0.97      0.99      0.98       496\n",
            "WALKING_DOWNSTAIRS       0.97      0.92      0.95       420\n",
            "  WALKING_UPSTAIRS       0.96      0.98      0.97       471\n",
            "\n",
            "          accuracy                           0.95      2947\n",
            "         macro avg       0.96      0.95      0.95      2947\n",
            "      weighted avg       0.96      0.95      0.95      2947\n",
            "\n",
            "Confusion Matrix:\n",
            " [[537   0   0   0   0   0]\n",
            " [  4 420  66   0   0   1]\n",
            " [  1  14 516   1   0   0]\n",
            " [  0   1   1 489   5   0]\n",
            " [  1   0   0  14 388  17]\n",
            " [  0   0   0   2   7 462]]\n",
            "Training time: 21.43s   Inference time: 0.01s\n",
            "--------------------------------------------------\n",
            "\n",
            "=== Summary ===\n",
            "                  Model  Accuracy  Train Time (s)  Inference Time (s)\n",
            "0      SVM (RBF kernel)  0.939260        1.972754            3.189891\n",
            "1         Random Forest  0.922633       12.306351            0.042625\n",
            "2  HistGradientBoosting  0.930777       17.888989            0.308819\n",
            "3  MLP (Feedforward NN)  0.954191       21.431262            0.013556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PINNs"
      ],
      "metadata": {
        "id": "P76zYxvEeEEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# ─── Assumes the following are already defined in your notebook:\n",
        "# raw_train (numpy array, shape [n_windows,128,6])\n",
        "# raw_test  (numpy array, shape [n_windows,128,6])\n",
        "# train_df, test_df with 'Activity' column\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Convert labels if not already done\n",
        "le = LabelEncoder()\n",
        "y_train_int = le.fit_transform(train_df['Activity'])\n",
        "y_test_int  = le.transform(test_df['Activity'])\n",
        "\n",
        "# Create PyTorch datasets\n",
        "X_tr = torch.tensor(raw_train, dtype=torch.float32)\n",
        "y_tr = torch.tensor(y_train_int, dtype=torch.long)\n",
        "X_te = torch.tensor(raw_test,  dtype=torch.float32)\n",
        "y_te = torch.tensor(y_test_int,  dtype=torch.long)\n",
        "\n",
        "train_ds = TensorDataset(X_tr, y_tr)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_ds  = TensorDataset(X_te, y_te)\n",
        "test_loader  = DataLoader(test_ds, batch_size=64)\n",
        "\n",
        "# … after DataLoader setup …\n",
        "\n",
        "# 3) Define PINN_HAR as before\n",
        "class PINN_HAR(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=64, num_layers=2, num_classes=6):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
        "                            batch_first=True, bidirectional=True)\n",
        "        self.fc_cls  = nn.Linear(2*hidden_size, num_classes)\n",
        "        self.fc_phys = nn.Linear(2*hidden_size, input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        cls_logits = self.fc_cls(out[:, -1, :])\n",
        "        phys_pred  = self.fc_phys(out)\n",
        "        return cls_logits, phys_pred\n",
        "\n",
        "# 4) Instantiate model with the correct channel dimension\n",
        "n_channels = raw_train.shape[2]   # should be 9\n",
        "n_classes  = len(le.classes_)     # should be 6 activities\n",
        "model = PINN_HAR(input_size=n_channels,\n",
        "                 hidden_size=64,\n",
        "                 num_layers=2,\n",
        "                 num_classes=n_classes).to(device)\n",
        "\n",
        "# … continue with optimizer, loss definitions, training loop, etc. …\n",
        "\n",
        "\n",
        "# # Instantiate model, optimizer, losses\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = PINN_HAR(input_size=6).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "mse_loss = nn.MSELoss()\n",
        "lambda_phys = 0.1\n",
        "\n",
        "# ─── 4) Training loop ─────────────────────────────────────────────────────────\n",
        "num_epochs = 50\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    total_cls, total_phys = 0.0, 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        Xb = X_batch.to(device)\n",
        "        yb = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, phys = model(Xb)\n",
        "\n",
        "        # Classification loss\n",
        "        loss_cls = ce_loss(logits, yb)\n",
        "\n",
        "        # Physics loss: second derivative approx\n",
        "        d2_phys = phys[:, 2:] - 2 * phys[:, 1:-1] + phys[:, :-2]  # (B,126,6)\n",
        "        acc_mid = Xb[:, 1:-1, :]                                 # (B,126,6)\n",
        "        loss_phys = mse_loss(d2_phys, acc_mid)\n",
        "\n",
        "        # Total loss\n",
        "        loss = loss_cls + lambda_phys * loss_phys\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_cls  += loss_cls.item()\n",
        "        total_phys += loss_phys.item()\n",
        "\n",
        "    avg_cls = total_cls / len(train_loader)\n",
        "    avg_phys = total_phys / len(train_loader)\n",
        "    print(f\"Epoch {epoch:2d} | CE Loss: {avg_cls:.4f} | Phys Loss: {avg_phys:.4f}\")\n",
        "\n",
        "# ─── Evaluation ───────────────────────────────────────────────────────────────\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        Xb = X_batch.to(device)\n",
        "        logits, _ = model(Xb)\n",
        "        preds = logits.argmax(dim=1).cpu()\n",
        "        correct += (preds == y_batch).sum().item()\n",
        "\n",
        "accuracy = correct / len(test_ds)\n",
        "print(f\"\\nTest Accuracy (PINN): {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N7G2YQheDDj",
        "outputId": "17d3a57a-8807-4fe7-dd8a-aeba8f74ed31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1 | CE Loss: 1.3295 | Phys Loss: 0.1675\n",
            "Epoch  2 | CE Loss: 0.9001 | Phys Loss: 0.1548\n",
            "Epoch  3 | CE Loss: 0.7265 | Phys Loss: 0.1456\n",
            "Epoch  4 | CE Loss: 0.6328 | Phys Loss: 0.1404\n",
            "Epoch  5 | CE Loss: 0.5891 | Phys Loss: 0.1366\n",
            "Epoch  6 | CE Loss: 0.4938 | Phys Loss: 0.1340\n",
            "Epoch  7 | CE Loss: 0.3613 | Phys Loss: 0.1324\n",
            "Epoch  8 | CE Loss: 0.2414 | Phys Loss: 0.1312\n",
            "Epoch  9 | CE Loss: 0.1845 | Phys Loss: 0.1296\n",
            "Epoch 10 | CE Loss: 0.1719 | Phys Loss: 0.1278\n",
            "Epoch 11 | CE Loss: 0.1440 | Phys Loss: 0.1265\n",
            "Epoch 12 | CE Loss: 0.1392 | Phys Loss: 0.1254\n",
            "Epoch 13 | CE Loss: 0.1254 | Phys Loss: 0.1238\n",
            "Epoch 14 | CE Loss: 0.1251 | Phys Loss: 0.1228\n",
            "Epoch 15 | CE Loss: 0.1222 | Phys Loss: 0.1224\n",
            "Epoch 16 | CE Loss: 0.1214 | Phys Loss: 0.1213\n",
            "Epoch 17 | CE Loss: 0.1221 | Phys Loss: 0.1206\n",
            "Epoch 18 | CE Loss: 0.1489 | Phys Loss: 0.1199\n",
            "Epoch 19 | CE Loss: 0.1194 | Phys Loss: 0.1189\n",
            "Epoch 20 | CE Loss: 0.1229 | Phys Loss: 0.1190\n",
            "Epoch 21 | CE Loss: 0.1143 | Phys Loss: 0.1208\n",
            "Epoch 22 | CE Loss: 0.1527 | Phys Loss: 0.1220\n",
            "Epoch 23 | CE Loss: 0.1211 | Phys Loss: 0.1199\n",
            "Epoch 24 | CE Loss: 0.1143 | Phys Loss: 0.1191\n",
            "Epoch 25 | CE Loss: 0.1196 | Phys Loss: 0.1181\n",
            "Epoch 26 | CE Loss: 0.1128 | Phys Loss: 0.1168\n",
            "Epoch 27 | CE Loss: 0.1078 | Phys Loss: 0.1157\n",
            "Epoch 28 | CE Loss: 0.1069 | Phys Loss: 0.1156\n",
            "Epoch 29 | CE Loss: 0.1062 | Phys Loss: 0.1142\n",
            "Epoch 30 | CE Loss: 0.1166 | Phys Loss: 0.1139\n",
            "Epoch 31 | CE Loss: 0.1065 | Phys Loss: 0.1136\n",
            "Epoch 32 | CE Loss: 0.1058 | Phys Loss: 0.1128\n",
            "Epoch 33 | CE Loss: 0.1037 | Phys Loss: 0.1114\n",
            "Epoch 34 | CE Loss: 0.1043 | Phys Loss: 0.1111\n",
            "Epoch 35 | CE Loss: 0.1020 | Phys Loss: 0.1104\n",
            "Epoch 36 | CE Loss: 0.1021 | Phys Loss: 0.1094\n",
            "Epoch 37 | CE Loss: 0.1044 | Phys Loss: 0.1086\n",
            "Epoch 38 | CE Loss: 0.1006 | Phys Loss: 0.1098\n",
            "Epoch 39 | CE Loss: 0.1028 | Phys Loss: 0.1080\n",
            "Epoch 40 | CE Loss: 0.1135 | Phys Loss: 0.1084\n",
            "Epoch 41 | CE Loss: 0.1096 | Phys Loss: 0.1077\n",
            "Epoch 42 | CE Loss: 0.1039 | Phys Loss: 0.1070\n",
            "Epoch 43 | CE Loss: 0.0989 | Phys Loss: 0.1058\n",
            "Epoch 44 | CE Loss: 0.0990 | Phys Loss: 0.1060\n",
            "Epoch 45 | CE Loss: 0.0959 | Phys Loss: 0.1050\n",
            "Epoch 46 | CE Loss: 0.0947 | Phys Loss: 0.1041\n",
            "Epoch 47 | CE Loss: 0.0969 | Phys Loss: 0.1036\n",
            "Epoch 48 | CE Loss: 0.1027 | Phys Loss: 0.1032\n",
            "Epoch 49 | CE Loss: 0.4053 | Phys Loss: 0.1252\n",
            "Epoch 50 | CE Loss: 0.1415 | Phys Loss: 0.1147\n",
            "\n",
            "Test Accuracy (PINN): 0.9043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers (use GPU)"
      ],
      "metadata": {
        "id": "0lnJffKEeJgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ─── 0) Prepare your data arrays ────────────────────────────────────────────\n",
        "# raw_train, raw_test: np.arrays, shape (n_windows, 128, n_channels)\n",
        "# train_df/test_df: pandas DataFrames with an 'Activity' column\n",
        "\n",
        "# Label encoding\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(train_df['Activity'].values)\n",
        "y_test  = le.transform(    test_df['Activity'].values)\n",
        "\n",
        "# TensorDatasets\n",
        "X_tr = torch.tensor(raw_train, dtype=torch.float32)\n",
        "y_tr = torch.tensor(y_train,    dtype=torch.long)\n",
        "X_te = torch.tensor(raw_test,  dtype=torch.float32)\n",
        "y_te = torch.tensor(y_test,    dtype=torch.long)\n",
        "\n",
        "train_ds = TensorDataset(X_tr, y_tr)\n",
        "test_ds  = TensorDataset(X_te, y_te)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=64)\n",
        "\n",
        "# ─── 1) Positional Encoding ─────────────────────────────────────────────────\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=128):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                        (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, d_model)\n",
        "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
        "\n",
        "# ─── 2) Transformer Classifier ───────────────────────────────────────────────\n",
        "class TransformerHAR(nn.Module):\n",
        "    def __init__(self, n_channels, d_model=64, n_heads=4,\n",
        "                 num_layers=2, num_classes=6, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(n_channels, d_model)\n",
        "        self.pos_enc     = PositionalEncoding(d_model, max_len=128)\n",
        "        encoder_layer    = nn.TransformerEncoderLayer(d_model=d_model,\n",
        "                                                      nhead=n_heads,\n",
        "                                                      dropout=dropout)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer,\n",
        "                                                 num_layers=num_layers)\n",
        "        self.classifier  = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, n_channels)\n",
        "        x = self.input_proj(x)           # → (batch, seq_len, d_model)\n",
        "        x = self.pos_enc(x)\n",
        "        x = x.permute(1, 0, 2)           # → (seq_len, batch, d_model)\n",
        "        enc = self.transformer(x)        # same shape\n",
        "        out = enc[-1]                    # last time-step (batch, d_model)\n",
        "        return self.classifier(out)      # logits (batch, num_classes)\n",
        "\n",
        "# ─── 3) Instantiate & train ─────────────────────────────────────────────────\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransformerHAR(n_channels=raw_train.shape[2],\n",
        "                       d_model=64,\n",
        "                       n_heads=4,\n",
        "                       num_layers=2,\n",
        "                       num_classes=len(le.classes_)).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, 21):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(Xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch:2d} | Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# ─── 4) Evaluate ─────────────────────────────────────────────────────────────\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in test_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        preds = model(Xb).argmax(dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "accuracy = correct / len(test_ds)\n",
        "print(f\"\\nTransformer HAR Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXY7DHzm0R_Y",
        "outputId": "2c1bb915-e476-45ff-fca8-fc26e283ae47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1 | Loss: 0.8457\n",
            "Epoch  2 | Loss: 0.3078\n",
            "Epoch  3 | Loss: 0.2082\n",
            "Epoch  4 | Loss: 0.1914\n",
            "Epoch  5 | Loss: 0.1782\n",
            "Epoch  6 | Loss: 0.1532\n",
            "Epoch  7 | Loss: 0.1426\n",
            "Epoch  8 | Loss: 0.1363\n",
            "Epoch  9 | Loss: 0.1396\n",
            "Epoch 10 | Loss: 0.1418\n",
            "Epoch 11 | Loss: 0.1478\n",
            "Epoch 12 | Loss: 0.1314\n",
            "Epoch 13 | Loss: 0.1226\n",
            "Epoch 14 | Loss: 0.1095\n",
            "Epoch 15 | Loss: 0.1107\n",
            "Epoch 16 | Loss: 0.1164\n",
            "Epoch 17 | Loss: 0.1084\n",
            "Epoch 18 | Loss: 0.1101\n",
            "Epoch 19 | Loss: 0.1304\n",
            "Epoch 20 | Loss: 0.1162\n",
            "\n",
            "Transformer HAR Test Accuracy: 0.8856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AR"
      ],
      "metadata": {
        "id": "CiT2tdTteOGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── 1) Load raw inertial windows (only once) ─────────────────────────────────\n",
        "import os, pandas as pd, numpy as np\n",
        "from statsmodels.tsa.ar_model import AutoReg\n",
        "\n",
        "def load_raw_windows(base_dir=\"/mnt/data/UCI_HAR_Dataset\", split=\"train\"):\n",
        "    folder   = os.path.join(base_dir, split, \"Inertial Signals\")\n",
        "    channels = [\"total_acc\",\"body_acc\",\"body_gyro\"]\n",
        "    axes     = [\"x\",\"y\",\"z\"]\n",
        "    mats     = []\n",
        "    for ch in channels:\n",
        "        for ax in axes:\n",
        "            fn = f\"{ch}_{ax}_{split}.txt\"\n",
        "            path = os.path.join(folder, fn)\n",
        "            arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
        "            mats.append(arr[..., np.newaxis])\n",
        "    return np.concatenate(mats, axis=2)\n",
        "\n",
        "raw_train = load_raw_windows(\"UCI_HAR_Dataset\",\"train\")  # (7352,128,6)\n",
        "raw_test  = load_raw_windows(\"UCI_HAR_Dataset\",\"test\")   # (2947,128,6)\n",
        "\n",
        "# ─── 2) Extract autoregressive (AR) features ──────────────────────────────────\n",
        "ar_lags = 4\n",
        "n_windows, _, n_ch = raw_train.shape\n",
        "\n",
        "def extract_ar_features(X, lags):\n",
        "    feats = np.zeros((X.shape[0], n_ch * (lags+1)))\n",
        "    for i in range(X.shape[0]):\n",
        "        v = []\n",
        "        for c in range(n_ch):\n",
        "            model = AutoReg(X[i, :, c], lags=lags, old_names=False).fit()\n",
        "            v.extend(model.params)   # [intercept, coeff1…coeff_lags]\n",
        "        feats[i] = v\n",
        "    return feats\n",
        "\n",
        "ar_train = extract_ar_features(raw_train, ar_lags)  # (7352,6*5=30)\n",
        "ar_test  = extract_ar_features(raw_test,  ar_lags)  # (2947,30)\n",
        "\n",
        "print(\"AR feature shapes:\", ar_train.shape, ar_test.shape)\n",
        "\n",
        "# ─── 3) Prepare final feature matrices & labels ──────────────────────────────\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(train_df[\"Activity\"])\n",
        "y_test  = le.transform(     test_df [\"Activity\"])\n",
        "\n",
        "# Use your 100 reduced features (replace with your actual list if not the first 100)\n",
        "feat_cols = [c for c in train_df.columns if c!=\"Activity\"][:100]\n",
        "\n",
        "X_train = np.hstack([train_df[feat_cols].values, ar_train])  # (7352,100+30=130)\n",
        "X_test  = np.hstack([ test_df [feat_cols].values, ar_test ])  # (2947,130)\n",
        "\n",
        "# ─── 4) Train & evaluate an SVM on AR features ───────────────────────────────\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "clf = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\")\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"AR+SVM Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZVAlaBieQxw",
        "outputId": "9eec0358-ca79-40ff-a958-4a684191eaa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
            "<ipython-input-14-f5f733358ee5>:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  arr  = pd.read_csv(path, delim_whitespace=True, header=None).values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AR feature shapes: (7352, 45) (2947, 45)\n",
            "AR+SVM Accuracy: 0.9127926705123854\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "            LAYING       1.00      1.00      1.00       537\n",
            "           SITTING       0.90      0.80      0.84       491\n",
            "          STANDING       0.83      0.92      0.87       532\n",
            "           WALKING       0.88      0.97      0.92       496\n",
            "WALKING_DOWNSTAIRS       0.98      0.88      0.93       420\n",
            "  WALKING_UPSTAIRS       0.91      0.90      0.90       471\n",
            "\n",
            "          accuracy                           0.91      2947\n",
            "         macro avg       0.92      0.91      0.91      2947\n",
            "      weighted avg       0.92      0.91      0.91      2947\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE"
      ],
      "metadata": {
        "id": "FOgWa3xJeTV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ─── Prerequisites: ensure raw_train/raw_test and train_df/test_df are loaded ──\n",
        "# raw_train/raw_test: numpy arrays (n_windows, 128, n_channels)\n",
        "# train_df/test_df: pandas DataFrames with 'Activity' column\n",
        "\n",
        "# Flatten each window to a vector of size 128*n_channels\n",
        "n_windows, seq_len, n_ch = raw_train.shape\n",
        "input_dim = seq_len * n_ch\n",
        "\n",
        "X_tr_flat = torch.tensor(raw_train.reshape(n_windows, -1), dtype=torch.float32)\n",
        "X_te_flat = torch.tensor(raw_test.reshape(raw_test.shape[0], -1), dtype=torch.float32)\n",
        "\n",
        "# ─── 1) Define VAE architecture ───────────────────────────────────────────────\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=512, latent_dim=32):\n",
        "        super().__init__()\n",
        "        # Encoder\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "        # Decoder\n",
        "        self.fc_dec1 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc_dec2 = nn.Linear(hidden_dim, input_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.relu(self.fc1(x))\n",
        "        return self.fc_mu(h), self.fc_logvar(h)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = self.relu(self.fc_dec1(z))\n",
        "        return self.sigmoid(self.fc_dec2(h))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        recon = self.decode(z)\n",
        "        return recon, mu, logvar\n",
        "\n",
        "# ─── 2) VAE loss function ─────────────────────────────────────────────────────\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    # Reconstruction loss (MSE)\n",
        "    recon_loss = nn.MSELoss(reduction='sum')(recon_x, x)\n",
        "    # KL divergence\n",
        "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + kld\n",
        "\n",
        "# ─── 3) Train VAE ─────────────────────────────────────────────────────────────\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vae = VAE(input_dim=input_dim, hidden_dim=512, latent_dim=32).to(device)\n",
        "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
        "\n",
        "train_ds = TensorDataset(X_tr_flat)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "\n",
        "vae.train()\n",
        "epochs = 50\n",
        "for epoch in range(1, epochs+1):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        x = batch[0].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon, mu, logvar = vae(x)\n",
        "        loss = vae_loss(recon, x, mu, logvar)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch:2d} | VAE Loss: {total_loss/len(train_loader.dataset):.4f}\")\n",
        "\n",
        "# ─── 4) Extract latent features ───────────────────────────────────────────────\n",
        "vae.eval()\n",
        "with torch.no_grad():\n",
        "    mu_train, _ = vae.encode(X_tr_flat.to(device))\n",
        "    mu_test,  _ = vae.encode(X_te_flat.to(device))\n",
        "latent_train = mu_train.cpu().numpy()\n",
        "latent_test  = mu_test.cpu().numpy()\n",
        "\n",
        "# ─── 5) Classify using SVM on latent features ────────────────────────────────\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(train_df['Activity'])\n",
        "y_test  = le.transform(test_df['Activity'])\n",
        "\n",
        "clf = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "clf.fit(latent_train, y_train)\n",
        "y_pred = clf.predict(latent_test)\n",
        "print(\"VAE+SVM Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW1-0bEaeVlx",
        "outputId": "b11217ad-bd8e-45d8-c51c-9f5d558172aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1 | VAE Loss: 104.2549\n",
            "Epoch  2 | VAE Loss: 80.6574\n",
            "Epoch  3 | VAE Loss: 79.3545\n",
            "Epoch  4 | VAE Loss: 78.8954\n",
            "Epoch  5 | VAE Loss: 78.6769\n",
            "Epoch  6 | VAE Loss: 78.2508\n",
            "Epoch  7 | VAE Loss: 77.9099\n",
            "Epoch  8 | VAE Loss: 77.6425\n",
            "Epoch  9 | VAE Loss: 77.2259\n",
            "Epoch 10 | VAE Loss: 76.5349\n",
            "Epoch 11 | VAE Loss: 75.6371\n",
            "Epoch 12 | VAE Loss: 75.3261\n",
            "Epoch 13 | VAE Loss: 74.9036\n",
            "Epoch 14 | VAE Loss: 74.4254\n",
            "Epoch 15 | VAE Loss: 74.0651\n",
            "Epoch 16 | VAE Loss: 73.9405\n",
            "Epoch 17 | VAE Loss: 73.4677\n",
            "Epoch 18 | VAE Loss: 73.3555\n",
            "Epoch 19 | VAE Loss: 73.1744\n",
            "Epoch 20 | VAE Loss: 72.9881\n",
            "Epoch 21 | VAE Loss: 72.8220\n",
            "Epoch 22 | VAE Loss: 72.6397\n",
            "Epoch 23 | VAE Loss: 72.6108\n",
            "Epoch 24 | VAE Loss: 72.4595\n",
            "Epoch 25 | VAE Loss: 72.4013\n",
            "Epoch 26 | VAE Loss: 72.2302\n",
            "Epoch 27 | VAE Loss: 72.1515\n",
            "Epoch 28 | VAE Loss: 72.3928\n",
            "Epoch 29 | VAE Loss: 72.0187\n",
            "Epoch 30 | VAE Loss: 72.1622\n",
            "Epoch 31 | VAE Loss: 72.0823\n",
            "Epoch 32 | VAE Loss: 71.7562\n",
            "Epoch 33 | VAE Loss: 71.7412\n",
            "Epoch 34 | VAE Loss: 71.5912\n",
            "Epoch 35 | VAE Loss: 71.5995\n",
            "Epoch 36 | VAE Loss: 71.4161\n",
            "Epoch 37 | VAE Loss: 71.6895\n",
            "Epoch 38 | VAE Loss: 71.5284\n",
            "Epoch 39 | VAE Loss: 71.3814\n",
            "Epoch 40 | VAE Loss: 71.2785\n",
            "Epoch 41 | VAE Loss: 71.2720\n",
            "Epoch 42 | VAE Loss: 71.3802\n",
            "Epoch 43 | VAE Loss: 71.1227\n",
            "Epoch 44 | VAE Loss: 71.0839\n",
            "Epoch 45 | VAE Loss: 71.0407\n",
            "Epoch 46 | VAE Loss: 71.0529\n",
            "Epoch 47 | VAE Loss: 70.9480\n",
            "Epoch 48 | VAE Loss: 70.8605\n",
            "Epoch 49 | VAE Loss: 70.8798\n",
            "Epoch 50 | VAE Loss: 70.8884\n",
            "VAE+SVM Accuracy: 0.8154054971157109\n"
          ]
        }
      ]
    }
  ]
}